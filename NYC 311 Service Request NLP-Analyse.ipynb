import subprocess
import sys

# 📦 Automatische Installation fehlender Bibliotheken
def install_if_missing(package):
    try:
        __import__(package)
    except ImportError:
        print(f"📦 Installiere {package}...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])

# Notwendige Pakete installieren
install_if_missing("pandas")
install_if_missing("numpy")
install_if_missing("nltk")
install_if_missing("gensim")
install_if_missing("matplotlib")
install_if_missing("seaborn")
install_if_missing("wordcloud")
install_if_missing("rapidfuzz")
install_if_missing("scikit-learn")

# Jetzt können alle Module importiert werden
import pandas as pd
import numpy as np
import re
import nltk
import gensim
import matplotlib.pyplot as plt
import seaborn as sns
import os
from wordcloud import WordCloud
from rapidfuzz import fuzz
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
from gensim.models import CoherenceModel
from gensim.corpora import Dictionary
from nltk.sentiment import SentimentIntensityAnalyzer

# Falls notwendig, NLTK-Resourcen herunterladen
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('vader_lexicon')

print("📥 Lade Daten...")
df = pd.read_csv("/Users/sarahmannes/Downloads/NLP Analyse/311_Service_Requests.csv", 
                 usecols=["Complaint Type", "Descriptor", "Location Type", "Incident Zip", "City", 
                          "Borough", "Resolution Description"])
df = df.dropna(subset=["Complaint Type", "Resolution Description"])  # Entferne fehlende Werte

print("🛠 Bereinige Texte...")
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))
custom_stopwords = {"complaint", "department", "police", "request", "issue", "service", "provided"}
stop_words.update(custom_stopwords)

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    tokens = word_tokenize(text)
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)

df["clean_text"] = df["Resolution Description"].astype(str).apply(preprocess_text)

df["clean_text"].to_csv("bereinigte_beschwerden.txt", index=False, header=False)
print("✅ Datei 'bereinigte_beschwerden.txt' wurde erfolgreich gespeichert!")

print("📊 Wandle Texte in numerische Vektoren um...")
vectorizer_bow = CountVectorizer(max_features=1000, ngram_range=(1,2), max_df=0.95, min_df=5)
bow_matrix = vectorizer_bow.fit_transform(df["clean_text"])

vectorizer_tfidf = TfidfVectorizer(max_features=1000, ngram_range=(1,2), max_df=0.95, min_df=5)
tfidf_matrix = vectorizer_tfidf.fit_transform(df["clean_text"])

with open("vektorisierung.txt", "w", encoding="utf-8") as file:
    file.write(f"BoW Shape: {bow_matrix.shape}\n")
    file.write(f"TF-IDF Shape: {tfidf_matrix.shape}\n")
print("✅ Datei 'vektorisierung.txt' wurde erfolgreich gespeichert!")

print("📑 Extrahiere Themen mit LDA & NMF...")
lda_model = LatentDirichletAllocation(n_components=5, max_iter=10, learning_method="online", random_state=42)
lda_model.fit(bow_matrix)

nmf_model = NMF(n_components=5, max_iter=200, random_state=42)
nmf_model.fit(tfidf_matrix)

print("📝 Bestimme Themennamen...")
predefined_topics = {
    "Traffic & Infrastructure": {"road", "street", "traffic", "parking", "sign", "bridge", "construction"},
    "Noise & Environment": {"noise", "garbage", "pollution", "waste", "recycling", "odor", "air"},
    "Safety & Crime": {"crime", "police", "theft", "vandalism", "assault", "robbery"},
    "Public Services": {"water", "electricity", "heating", "public", "transport", "service"},
    "Social Issues": {"homeless", "shelter", "drug", "abuse", "support", "mental", "health"}
}

def name_topic(topic_words):
    best_match = "Miscellaneous Issues"
    best_score = 0
    topic_words_str = ", ".join(topic_words)

    for topic_name, keywords in predefined_topics.items():
        keywords_str = ", ".join(keywords)
        score = fuzz.partial_ratio(topic_words_str, keywords_str)
        if score > best_score:
            best_match = topic_name
            best_score = score
    return best_match

def save_named_topics(model, feature_names, filename, n_words=10):
    with open(filename, "w", encoding="utf-8") as file:
        for topic_idx, topic in enumerate(model.components_):
            topic_words = [feature_names[i] for i in topic.argsort()[:-n_words - 1:-1]]
            topic_name = name_topic(topic_words)
            file.write(f"🔹 Thema {topic_idx+1} ({topic_name}): {' | '.join(topic_words)}\n")
    print(f"✅ Datei '{filename}' wurde erfolgreich gespeichert!")

save_named_topics(lda_model, vectorizer_bow.get_feature_names_out(), "lda_themen.txt")
save_named_topics(nmf_model, vectorizer_tfidf.get_feature_names_out(), "nmf_themen.txt")

print("📊 Berechne Coherence Score...")
texts = [text.split() for text in df["clean_text"]]
dictionary = Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

lda_gensim = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, passes=10)
coherence_model_lda = CoherenceModel(model=lda_gensim, texts=texts, dictionary=dictionary, coherence='c_v')

coherence_score = f"Coherence Score für LDA: {coherence_model_lda.get_coherence():.4f}\n"
with open("coherence_score.txt", "w", encoding="utf-8") as file:
    file.write(coherence_score)
print("✅ Datei 'coherence_score.txt' wurde erfolgreich gespeichert!")

print("📊 Erstelle LDA-Visualisierung...")
plt.figure(figsize=(10, 5))
sns.barplot(x=[f'Thema {i+1}' for i in range(5)], y=lda_model.components_.sum(axis=1))
plt.title("Anzahl der Wörter pro Thema (LDA)")
plt.xlabel("Thema")
plt.ylabel("Wort-Häufigkeit")
plt.savefig("lda_visualisierung.png")
plt.show()
print("✅ Datei 'lda_visualisierung.png' wurde erfolgreich gespeichert!")

print("📊 Erstelle Sentiment-Verteilung...")
sia = SentimentIntensityAnalyzer()
df["sentiment"] = df["clean_text"].apply(lambda x: sia.polarity_scores(x)['compound'])

df[["clean_text", "sentiment"]].to_csv("sentiment_analyse.txt", index=False)
print("✅ Datei 'sentiment_analyse.txt' wurde erfolgreich gespeichert!")

plt.figure(figsize=(8,5))
sns.histplot(df["sentiment"], bins=30, kde=True)
plt.title("Sentiment-Verteilung der Beschwerden")
plt.xlabel("Sentiment-Score")
plt.ylabel("Anzahl der Beschwerden")
plt.savefig("sentiment_verteilung.png")
plt.show()
print("✅ Datei 'sentiment_verteilung.png' wurde erfolgreich gespeichert!")

print("🚀 Analyse abgeschlossen!")
