import subprocess
import sys

# 📦 Automatische Installation fehlender Bibliotheken
def install_if_missing(package):
    try:
        __import__(package)
    except ImportError:
        print(f"📦 Installiere {package}...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])

# Notwendige Pakete installieren
install_if_missing("pandas")
install_if_missing("numpy")
install_if_missing("nltk")
install_if_missing("gensim")
install_if_missing("matplotlib")
install_if_missing("seaborn")
install_if_missing("wordcloud")
install_if_missing("scikit-learn")

# 📚 Importieren der Bibliotheken
import pandas as pd
import numpy as np
import re
import nltk
import gensim
import matplotlib.pyplot as plt
import seaborn as sns
import os
from wordcloud import WordCloud
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
from gensim.models import CoherenceModel
from gensim.corpora import Dictionary
from nltk.sentiment import SentimentIntensityAnalyzer

# Falls notwendig, NLTK-Resourcen herunterladen
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('vader_lexicon')

# 📥 Daten laden
print("📥 Lade Daten...")
df = pd.read_csv("/Users/sarahmannes/Downloads/NLP Analyse/Reviews.csv", usecols=["Text"])
df = df.dropna(subset=["Text"])  # Entferne fehlende Werte

# 🛠 Textvorbereitung
print("🛠 Bereinige Texte...")
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))
custom_stopwords = {"br", "product", "amazon", "buy", "purchase", "order", "get"}
stop_words.update(custom_stopwords)

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    tokens = word_tokenize(text)
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)

df["clean_text"] = df["Text"].astype(str).apply(preprocess_text)

df["clean_text"].to_csv("bereinigte_rezensionen.txt", index=False, header=False)
print("✅ Datei 'bereinigte_rezensionen.txt' wurde erfolgreich gespeichert!")

# 📊 Vektorisierung (BoW & TF-IDF)
print("📊 Wandle Texte in numerische Vektoren um...")
vectorizer_bow = CountVectorizer(max_features=1000, ngram_range=(1,2), max_df=0.95, min_df=5)
bow_matrix = vectorizer_bow.fit_transform(df["clean_text"])

vectorizer_tfidf = TfidfVectorizer(max_features=1000, ngram_range=(1,2), max_df=0.95, min_df=5)
tfidf_matrix = vectorizer_tfidf.fit_transform(df["clean_text"])

with open("vektorisierung.txt", "w", encoding="utf-8") as file:
    file.write(f"BoW Shape: {bow_matrix.shape}\n")
    file.write(f"TF-IDF Shape: {tfidf_matrix.shape}\n")
print("✅ Datei 'vektorisierung.txt' wurde erfolgreich gespeichert!")

# 📑 Themenextraktion (LDA & NMF)
print("📑 Extrahiere Themen mit LDA & NMF...")
lda_model = LatentDirichletAllocation(n_components=5, max_iter=10, learning_method="online", random_state=42)
lda_model.fit(bow_matrix)

nmf_model = NMF(n_components=5, max_iter=200, random_state=42)
nmf_model.fit(tfidf_matrix)

# 📝 Themenbenennung verbessern
print("📝 Bestimme Themennamen...")
predefined_topics = {
    "Taste & Quality": {"flavor", "taste", "delicious", "good", "bad", "sweet", "bitter", "fresh"},
    "Product Ingredients": {"organic", "sugar", "protein", "fat", "carbs", "gluten", "preservative"},
    "Packaging & Delivery": {"box", "bag", "packaging", "sealed", "damaged", "fresh"},
    "Customer Service": {"refund", "return", "support", "contact", "replacement", "problem"},
    "Price & Value": {"cheap", "expensive", "value", "deal", "price", "worth"},
    "Pet Products": {"dog", "cat", "treat", "pet", "chew", "toy", "bone", "feed"}

}

def name_topic(topic_words):
    topic_scores = {topic: len(set(topic_words) & keywords) for topic, keywords in predefined_topics.items()}
    best_topic = max(topic_scores, key=topic_scores.get)
    return best_topic if topic_scores[best_topic] > 1 else "Miscellaneous"

def save_named_topics(model, feature_names, filename, n_words=10):
    with open(filename, "w", encoding="utf-8") as file:
        for topic_idx, topic in enumerate(model.components_):
            topic_words = [feature_names[i] for i in topic.argsort()[:-n_words - 1:-1]]
            topic_name = name_topic(topic_words)
            file.write(f"🔹 Thema {topic_idx+1} ({topic_name}): {' | '.join(topic_words)}\n")
    print(f"✅ Datei '{filename}' wurde erfolgreich gespeichert!")

save_named_topics(lda_model, vectorizer_bow.get_feature_names_out(), "lda_themen.txt")
save_named_topics(nmf_model, vectorizer_tfidf.get_feature_names_out(), "nmf_themen.txt")

# 📊 Coherence Score berechnen
print("📊 Berechne Coherence Score...")
texts = [text.split() for text in df["clean_text"]]
dictionary = Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

lda_gensim = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, passes=10)
coherence_model_lda = CoherenceModel(model=lda_gensim, texts=texts, dictionary=dictionary, coherence='c_v')

coherence_score = f"Coherence Score für LDA: {coherence_model_lda.get_coherence():.4f}\n"
with open("coherence_score.txt", "w", encoding="utf-8") as file:
    file.write(coherence_score)
print("✅ Datei 'coherence_score.txt' wurde erfolgreich gespeichert!")

# 📊 LDA-Visualisierung mit Themenbezeichnungen
print("📊 Erstelle LDA-Visualisierung...")
topic_labels = [name_topic([vectorizer_bow.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]]) for topic in lda_model.components_]

plt.figure(figsize=(10, 5))
sns.barplot(x=topic_labels, y=lda_model.components_.sum(axis=1))
plt.title("Anzahl der Wörter pro Thema (LDA)")
plt.xlabel("Thema")
plt.ylabel("Wort-Häufigkeit")
plt.xticks(rotation=30, ha='right')
plt.savefig("lda_visualisierung.png")
plt.show()
print("✅ Datei 'lda_visualisierung.png' wurde erfolgreich gespeichert!")

# 📊 Sentiment-Analyse
print("📊 Erstelle Sentiment-Verteilung...")
sia = SentimentIntensityAnalyzer()
df["sentiment"] = df["clean_text"].apply(lambda x: sia.polarity_scores(x)['compound'])

df[["clean_text", "sentiment"]].to_csv("sentiment_analyse.txt", index=False)
print("✅ Datei 'sentiment_analyse.txt' wurde erfolgreich gespeichert!")

plt.figure(figsize=(8,5))
sns.histplot(df["sentiment"], bins=30, kde=True)
plt.title("Sentiment-Verteilung der Rezensionen")
plt.xlabel("Sentiment-Score")
plt.ylabel("Anzahl der Bewertungen")
plt.savefig("sentiment_verteilung.png")
plt.show()
print("✅ Datei 'sentiment_verteilung.png' wurde erfolgreich gespeichert!")

print("🚀 Analyse abgeschlossen!")
