import subprocess
import sys

# ðŸ“¦ Automatische Installation fehlender Bibliotheken
def install_if_missing(package):
    try:
        __import__(package)
    except ImportError:
        print(f"ðŸ“¦ Installiere {package}...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])

# Notwendige Pakete installieren
for pkg in ["pandas", "numpy", "nltk", "gensim", "matplotlib", "seaborn", "wordcloud", "rapidfuzz", "scikit-learn"]:
    install_if_missing(pkg)

# Module importieren
import pandas as pd
import numpy as np
import re
import nltk
import gensim
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from rapidfuzz import fuzz
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
from gensim.models import CoherenceModel
from gensim.corpora import Dictionary
from nltk.sentiment import SentimentIntensityAnalyzer

# ðŸ”½ NLTK-Resourcen
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('vader_lexicon')

# ðŸ“¥ Lade Daten
print("ðŸ“¥ Lade Daten...")
df = pd.read_csv("/Users/sarahmannes/Downloads/NLP Analyse/Reviews.csv", usecols=["Text"])
df = df.dropna(subset=["Text"])

# ðŸ›  Bereinige Texte
print("ðŸ›  Bereinige Texte...")
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))
custom_stopwords = {"br", "one", "get", "really", "good", "love", "use", "product", "would"}
stop_words.update(custom_stopwords)

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'<br\s*/?>', ' ', text)
    text = re.sub(r'\bbr\b', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    tokens = word_tokenize(text)
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)

df["clean_text"] = df["Text"].astype(str).apply(preprocess_text)
df["clean_text"].to_csv("bereinigte_reviews.txt", index=False, header=False)
print("âœ… Datei 'bereinigte_reviews.txt' wurde erfolgreich gespeichert!")

# ðŸ“Š Vektorisierung
print("ðŸ“Š Wandle Texte in numerische Vektoren um...")
vectorizer_bow = CountVectorizer(max_features=1000, ngram_range=(1,2), max_df=0.95, min_df=5)
bow_matrix = vectorizer_bow.fit_transform(df["clean_text"])

vectorizer_tfidf = TfidfVectorizer(max_features=1000, ngram_range=(1,2), max_df=0.95, min_df=5)
tfidf_matrix = vectorizer_tfidf.fit_transform(df["clean_text"])

with open("vektorisierung.txt", "w", encoding="utf-8") as file:
    file.write(f"BoW Shape: {bow_matrix.shape}\n")
    file.write(f"TF-IDF Shape: {tfidf_matrix.shape}\n")
print("âœ… Datei 'vektorisierung.txt' wurde erfolgreich gespeichert!")

# ðŸ“‘ Themenextraktion
print("ðŸ“‘ Extrahiere Themen mit LDA & NMF...")
lda_model = LatentDirichletAllocation(n_components=5, max_iter=10, learning_method="online", random_state=42)
lda_model.fit(bow_matrix)

nmf_model = NMF(n_components=5, max_iter=200, random_state=42)
nmf_model.fit(tfidf_matrix)

# ðŸ“ Themenbenennung
print("ðŸ“ Bestimme Themennamen...")
predefined_topics = {
    "Pet Products": {"dog", "cat", "pet", "animal", "treat", "puppy", "kibble"},
    "Taste & Quality": {"taste", "flavor", "sweet", "texture", "chocolate", "rich", "smooth"},
    "Packaging & Delivery": {"package", "shipping", "box", "bag", "arrived", "delivered", "ordered"},
    "Price & Value": {"price", "expensive", "cheap", "affordable", "deal", "worth", "cost", "value"},
    "Nutrition & Ingredients": {"ingredient", "sugar", "protein", "calorie", "organic", "natural", "salt", "healthy"}
}

def name_topic(topic_words):
    best_match = "Miscellaneous"
    best_score = 0
    topic_words_str = " ".join(topic_words)
    for topic_name, keywords in predefined_topics.items():
        keywords_str = " ".join(keywords)
        score = fuzz.token_sort_ratio(topic_words_str, keywords_str)
        if score > best_score:
            best_match = topic_name
            best_score = score
    return best_match

def save_named_topics(model, feature_names, filename, n_words=10):
    with open(filename, "w", encoding="utf-8") as file:
        for topic_idx, topic in enumerate(model.components_):
            topic_words = [feature_names[i] for i in topic.argsort()[:-n_words - 1:-1]]
            topic_name = name_topic(topic_words)
            file.write(f"\U0001F539 Thema {topic_idx+1} ({topic_name}): {' | '.join(topic_words)}\n")
    print(f"âœ… Datei '{filename}' wurde erfolgreich gespeichert!")

save_named_topics(lda_model, vectorizer_bow.get_feature_names_out(), "lda_themen.txt")
save_named_topics(nmf_model, vectorizer_tfidf.get_feature_names_out(), "nmf_themen.txt")

# ðŸ“Š Coherence Score
print("ðŸ“Š Berechne Coherence Score...")
texts = [text.split() for text in df["clean_text"]]
dictionary = Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

lda_gensim = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, passes=10)
coherence_model_lda = CoherenceModel(model=lda_gensim, texts=texts, dictionary=dictionary, coherence='c_v')
coherence_score = f"Coherence Score fÃ¼r LDA: {coherence_model_lda.get_coherence():.4f}\n"

nmf_topics = []
feature_names_tfidf = vectorizer_tfidf.get_feature_names_out()
for topic in nmf_model.components_:
    top_words = [feature_names_tfidf[i] for i in topic.argsort()[:-10 - 1:-1]]
    nmf_topics.append(top_words)

coherence_model_nmf = CoherenceModel(topics=nmf_topics, texts=texts, dictionary=dictionary, coherence='c_v')
coherence_score_nmf = f"Coherence Score fÃ¼r NMF: {coherence_model_nmf.get_coherence():.4f}\n"

with open("coherence_score.txt", "w", encoding="utf-8") as file:
    file.write(coherence_score)
    file.write(coherence_score_nmf)
print("âœ… Datei 'coherence_score.txt' wurde erfolgreich aktualisiert!")

# ðŸ“Š LDA Visualisierung
print("ðŸ“Š Erstelle LDA-Visualisierung...")
topic_names = []
for topic in lda_model.components_:
    top_words = [vectorizer_bow.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]]
    topic_names.append(name_topic(top_words))

plt.figure(figsize=(10, 5))
sns.barplot(x=topic_names, y=lda_model.components_.sum(axis=1))
plt.title("Wortanzahl pro Thema (LDA)")
plt.xlabel("Thema")
plt.ylabel("HÃ¤ufigkeit")
plt.xticks(rotation=30)
plt.tight_layout()
plt.savefig("lda_visualisierung.png")
plt.show()
print("âœ… Datei 'lda_visualisierung.png' wurde erfolgreich gespeichert!")

# ðŸ“Š Wordcloud pro Thema
print("ðŸ“Š Erstelle Wordclouds fÃ¼r LDA-Themen...")
for idx, topic in enumerate(lda_model.components_):
    top_words = {vectorizer_bow.get_feature_names_out()[i]: topic[i] for i in topic.argsort()[:-30-1:-1]}
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(top_words)
    plt.figure(figsize=(8,4))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis('off')
    plt.title(f"Thema {idx+1}")
    plt.tight_layout()
    plt.savefig(f"wordcloud_thema_{idx+1}.png")
    plt.close()
print("âœ… Wordclouds gespeichert!")

# ðŸ“Š Sentiment-Analyse
print("ðŸ“Š Erstelle Sentiment-Verteilung...")
sia = SentimentIntensityAnalyzer()
df["sentiment"] = df["clean_text"].apply(lambda x: sia.polarity_scores(x)['compound'])
df[["clean_text", "sentiment"]].to_csv("sentiment_analyse.txt", index=False)
print("âœ… Datei 'sentiment_analyse.txt' wurde erfolgreich gespeichert!")

plt.figure(figsize=(8,5))
sns.histplot(df["sentiment"], bins=30, kde=True)
plt.title("Sentiment-Verteilung")
plt.xlabel("Sentiment-Score")
plt.ylabel("Anzahl Texte")
plt.savefig("sentiment_verteilung.png")
plt.show()
print("âœ… Datei 'sentiment_verteilung.png' wurde erfolgreich gespeichert!")

# ðŸ“Š ErgebnisÃ¼bersicht
print("ðŸ“Š Erstelle zusammenfassende Ergebnisdatei...")
num_texts = len(df)
num_positive = (df["sentiment"] > 0.05).sum()
num_negative = (df["sentiment"] < -0.05).sum()
num_neutral = num_texts - num_positive - num_negative

with open("ergebnisse_uebersicht.txt", "w", encoding="utf-8") as file:
    file.write(f"Anzahl Texte: {num_texts}\n")
    file.write(f"BoW Shape: {bow_matrix.shape}\n")
    file.write(f"TF-IDF Shape: {tfidf_matrix.shape}\n")
    file.write(coherence_score)
    file.write(coherence_score_nmf)
    file.write(f"Positive Texte: {num_positive}\n")
    file.write(f"Negative Texte: {num_negative}\n")
    file.write(f"Neutrale Texte: {num_neutral}\n")
print("âœ… Datei 'ergebnisse_uebersicht.txt' wurde erfolgreich gespeichert!")

print("ðŸš€ Analyse abgeschlossen!")
